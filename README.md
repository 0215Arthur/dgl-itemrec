# Item-based recommendation example with DGL

## Model description

We currently implement two models on MovieLens-1M.

**KNN**

The first one is an unsupervised item-embedding learning model, where the
item embeddings are computed from PinSage on the user-item interaction
graph.  The item embeddings are optimized so that the dot product of
two item embeddings are maximized if they are interacted by the same user,
and minimized otherwise.

Once learned, top-K recommendation are generated by comparing the item
embedding with the latest item the user has interacted with, and selecting
K items with the largest dot product.

**FISM**

The second one is based on FISM, except that the item embeddings are again
computed by PinSage.

### Requirements

This program requires my own fork of DGL to run: https://github.com/BarclayII/dgl

In addition, it also requires the following Python packages:

* PyTorch
* tqdm
* pandas
* sh
* scikit-learn

To get away from the trouble of environment configuration, one can instead build
a Docker container from the Dockerfile in this repo:

```
$ nvidia-docker build -t dgl-itemrec-image - < Dockerfile
```

And run the trainer of the first model by mounting the repository and the data directories
as Docker volumes:

```
$ nvidia-docker run --rm \
>   -v /path/to/this/repo:/path/to/repo/in/container \
>   -v /path/to/data:/path/to/data/in/container \
>   -t dgl-itemrec-image python /path/to/repo/in/container/main_knn.py \
>   --your-arguments
```

If one wishes to train the second model, change `main_knn.py` to `main_fism.py`.

### LibMF pretraining

If one wishes to pretrain the item embeddings using LibMF, one needs to first run

```
git submodule update --init
```

and follow the instructions in `libmf` directory to build the `mf-train` executable.

### Arguments

* `--n-epoch`: number of epochs

#### Hyperparameters

##### PinSAGE hyperparameters

* `--batch-size`: batch size
* `--feature-size`: dimension of representations learned for each item
* `--id-as-feature`: whether to treat item ID as a feature.
* `--n-layers`: number of layers (default 1) (can be 0)
* `--n-traces`: number of random walk paths to sample for PinSAGE neighbors
* `--trace-len`: length of random walk paths
* `--n-neighbors`: number of neighbors for each node to sample

##### Common training hyperparameters

* `--n-negs`: number of negative examples to sample for loss computation
* `--weight-decay`: weight decay
* `--lr`: learning rate
* `--id-as-feature`: whether to encode ID as a feature (one-hot encoding)
* `--pretrain`: whether to use LibMF to pretrain the item embeddings
  (requires `--id-as-feature`; see **LibMF pretraining** section above)

##### FISM-specific training hyperparameters

* `--alpha`: Alpha in the FISM paper, ranging from 0 to 1.

##### KNN-specific training hyperparameters

* `--margin`: Margin for ranking loss, non-negative.
* `--max-c`: The maximum value of weighting the loss term of a training example.  By default, each training example
  is weighted by the number of cooccurrences.  Assign this with a value to set a maximum.
* `--neg-by-freq`: Whether to sample negative examples with probability proportional to item occurrences rather than uniformly.
* `--neg-freq-max`: Set this to clamp the maximum value of unnormalized probability of negative sampling.  Requires `--neg-by-freq`.
* `--neg-freq-min`: Likewise, but the minimum.  Requires `--neg-by-freq`.

##### Other training configurations

* `--n-epoch`: number of epochs to run
* `--iters-per-epoch`: number of training iterations to run per epoch, or
  number of training iterations to run before validating once.
* `--n-workers`: number of data loading workers; speeds up PinSAGE neighbor sampling
* `--dataset`: either `movielens` for MovieLens 1M or `bx` for BookCrossing.
* `--data-pickle`: the pickle file that caches the preprocessed dataset object.
* `--data-path`: path that points to the raw dataset.
* `--model-path`: path to save the model.

### Grid/Random search tool

We provide another script `manual_automl.py` that performs grid or random search:

```
python manual_automl.py
```

One could specify the hyperparameter grid, the GPUs to run on, number of combinations to try,
and the script to tune hyperparameters on in the script file.
